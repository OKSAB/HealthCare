{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR_ivv1YPcAo"
      },
      "source": [
        "This colab notebook demonstrates how to make use of api calls by passing on user inputs to endpoints as inputs and receiving responses which can be rendered on the frontend.\n",
        "\n",
        "The methods herein make the following assumptions:\n",
        "\n",
        "\n",
        "*   You have successfully mounted google drive for use in google colab\n",
        "*   You have successfully cloned the repository and placed it in a folder named 'ChatDoctorProject', which is contained in GoogleDrive.\n",
        "After successfully cloning the repository, the file structure should appear as : '/drive/MyDrive/ChatDoctorProject/ChatDoctor/'\n",
        "*   You have successfully downloaded the model files into a folder named 'pretrained', whose parent folder is 'ChatDoctor'. The model files will therefore be contained in '/drive/MyDrive/ChatDoctorProject/ChatDoctor/pretrained/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS1aE0jFDzuH",
        "outputId": "0f9a5d62-59df-48a3-fa17-af6c8d8813fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtUjecdoUBVM",
        "outputId": "4fdc961c-107a-4008-f96a-3eb92c84951c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.4 starlette-0.46.2 uvicorn-0.34.1\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Install required dependencies \"\"\"\n",
        "!pip install fastapi uvicorn nest_asyncio safetensors pyngrok transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KTkmwyrUq-S",
        "outputId": "5c6b2f1f-4f7c-40e1-ebcb-bd409e7f3a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ChatDoctorProject/ChatDoctor\n"
          ]
        }
      ],
      "source": [
        "\"\"\" cd into the ChatDoctor directory \"\"\"\n",
        "%cd /content/drive/MyDrive/ChatDoctorProject/ChatDoctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "479b220e38894bf9bbd3a767905b060a",
            "28058875d77c4e95b9f3df28748bb768",
            "8db56721efda447fb634d251d6ce782c",
            "01b1ffa54d69452094d9f6b6c56a13be",
            "e478a480e20240dda6087656e81289e3",
            "a5f5a24949404250adabe8fed05a0136",
            "d39bf9dd696547cbba857c2a08b15b9b",
            "4dbfee44dd7745c6b8166da31b6b8faa",
            "e15307e522b64a5e9b782053acfda2a1",
            "fc1fc7716d704f718dc40d86e6ebf9ca",
            "f5f3587932bc4a6d9e31c1e729c97eeb"
          ]
        },
        "id": "FeH35n3SVGRr",
        "outputId": "beb7b383-4d23-47e1-b663-2362bb182c90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:609: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "479b220e38894bf9bbd3a767905b060a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\" Create a function that loads the model for use in consequent cells \"\"\"\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "model=None\n",
        "tokenizer=None\n",
        "generator=None\n",
        "\n",
        "# Create a function to load the model\n",
        "def load_model(model_name=\"./pretrained/\", eight_bit=0, device_map=\"auto\"):\n",
        "    global model, tokenizer, generator\n",
        "\n",
        "    if device_map == \"zero\":\n",
        "        device_map = \"balanced_low_0\"\n",
        "\n",
        "    tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
        "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        load_in_8bit=False,\n",
        "        cache_dir=\"cache\"\n",
        "    ).cuda()\n",
        "\n",
        "    generator = model.generate\n",
        "\n",
        "load_model() #loads the model once and makes the variables (model, tokenizer, generator) available globally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo_ewTiGWG9h"
      },
      "outputs": [],
      "source": [
        "\"\"\" Defines a function that takes in a users prompt, generates and returns a response \"\"\"\n",
        "\n",
        "# Define your system prompt for natural remedy advice.\n",
        "system_prompt = (\n",
        "    \"You are a highly knowledgeable medical assistant who exclusively provides natural remedy advice \"\n",
        "    \"using botanical and herbal treatments. You NEVER suggest conventional pharmaceutical treatments. \"\n",
        "    \"Answer the user's questions with natural remedies only.\\n\\n\"\n",
        ")\n",
        "\n",
        "# Start the conversation history with an initial greeting that reinforces natural remedy focus.\n",
        "First_chat = \"ChatDoctor: I am ChatDoctor. I only provide natural remedy medical suggestions. What natural remedy medical questions do you have?\"\n",
        "\n",
        "history = []\n",
        "history.append(First_chat)\n",
        "async def chat(prompt:str)->str:\n",
        "  invitation = \"ChatDoctor: \"\n",
        "  human_invitation = \"Patient: \"\n",
        "\n",
        "  history.append(human_invitation + prompt)\n",
        "\n",
        "  fulltext = (\n",
        "        system_prompt +                     # <-- Added system prompt here.\n",
        "        \"\\n\\n\".join(history) + \"\\n\\n\" +\n",
        "        invitation\n",
        "    )\n",
        "\n",
        "  generated_text = \"\"\n",
        "  gen_in = tokenizer(fulltext, return_tensors=\"pt\").input_ids.cuda()\n",
        "  in_tokens = len(gen_in)\n",
        "  with torch.no_grad():\n",
        "          generated_ids = generator(\n",
        "              gen_in,\n",
        "              max_new_tokens=200,\n",
        "              use_cache=True,\n",
        "              pad_token_id=tokenizer.eos_token_id,\n",
        "              num_return_sequences=1,\n",
        "              do_sample=True,\n",
        "              repetition_penalty=1.1, # 1.0 means 'off'. unfortunately if we penalize it it will not output Sphynx:\n",
        "              temperature=0.5, # default: 1.0\n",
        "              top_k = 50, # default: 50\n",
        "              top_p = 1.0, # default: 1.0\n",
        "              early_stopping=True,\n",
        "          )\n",
        "          generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "          text_without_prompt = generated_text[len(fulltext):]\n",
        "  response = text_without_prompt\n",
        "  response = response.split(human_invitation)[0]\n",
        "  response.strip()\n",
        "  history.append(invitation + response)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1N3OqKKY5xD"
      },
      "outputs": [],
      "source": [
        "\"\"\" Create a FastAPI application \"\"\"\n",
        "# Create a fastapi application instance\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app=FastAPI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cweqh4kTZYBa",
        "outputId": "bc99db0a-a67d-4ffc-d0fc-e23d54796605"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Create more endpoints as required '"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" Create the required endpoints \"\"\"\n",
        "\n",
        "@app.get('/') #default homepage displayed on the home route\n",
        "def home():\n",
        "  print(\"Homepage\")\n",
        "  return {\"detail\":\"This is the ChatDoctor homepage..\"}\n",
        "\n",
        "@app.post('/ask') #end point used to pass a prompt and and return the LLMs response\n",
        "async def ask(data:dict):\n",
        "  \"\"\"\n",
        "  This endpoint takes in a dict, passed as a json object,\n",
        "  calls the chat function as described above and\n",
        "  awaits for the response before returning the response as a json object\n",
        "  \"\"\"\n",
        "  response=await chat(prompt=data[\"prompt\"])\n",
        "  return {\"detail\":response}\n",
        "\n",
        "\"\"\" Create more endpoints as required \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fHsKbocfbS_l",
        "outputId": "5dbbed56-57f0-4573-dd38-5c1a38ee7ae9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [363]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"https://c718-34-87-79-119.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2001:16a2:f471:e800:b861:500e:1dfa:e7fc:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     2001:16a2:f471:e800:b861:500e:1dfa:e7fc:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     2001:16a2:f471:e800:b861:500e:1dfa:e7fc:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     2001:16a2:f471:e800:b861:500e:1dfa:e7fc:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [363]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Run the app to access the endpoints on any device using ngrok \"\"\"\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "!ngrok config add-authtoken \"2vFXG8AxzFYsAgAiGy94BkVSblY_w6BbWYnTQRCjWm1KgNiw\"\n",
        "\n",
        "# Allow running uvicorn in Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        " # Setup a tunnel to the app\n",
        "public_url = ngrok.connect(8000)\n",
        "print(public_url) #use this url on any program(frontend-application, postman etc..) to access the apis\n",
        "\n",
        "# run the app using uvicorn\n",
        "uvicorn.run(app=app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01b1ffa54d69452094d9f6b6c56a13be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc1fc7716d704f718dc40d86e6ebf9ca",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f3587932bc4a6d9e31c1e729c97eeb",
            "value": " 3/3 [08:58&lt;00:00, 171.69s/it]"
          }
        },
        "28058875d77c4e95b9f3df28748bb768": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f5a24949404250adabe8fed05a0136",
            "placeholder": "​",
            "style": "IPY_MODEL_d39bf9dd696547cbba857c2a08b15b9b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "479b220e38894bf9bbd3a767905b060a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28058875d77c4e95b9f3df28748bb768",
              "IPY_MODEL_8db56721efda447fb634d251d6ce782c",
              "IPY_MODEL_01b1ffa54d69452094d9f6b6c56a13be"
            ],
            "layout": "IPY_MODEL_e478a480e20240dda6087656e81289e3"
          }
        },
        "4dbfee44dd7745c6b8166da31b6b8faa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db56721efda447fb634d251d6ce782c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dbfee44dd7745c6b8166da31b6b8faa",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e15307e522b64a5e9b782053acfda2a1",
            "value": 3
          }
        },
        "a5f5a24949404250adabe8fed05a0136": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39bf9dd696547cbba857c2a08b15b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e15307e522b64a5e9b782053acfda2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e478a480e20240dda6087656e81289e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f3587932bc4a6d9e31c1e729c97eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc1fc7716d704f718dc40d86e6ebf9ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}